{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** home_dir is ** : /Users/ganesh/Documents/GitHub/DLAudio/GS_Valerio_SoundofAI \n",
      "\n",
      " ** home_dir.ls() ** \n",
      " \n",
      ".DS_Store\n",
      "Spectrogram+MelSpectrogram.ipynb\n",
      "MFCC.ipynb\n",
      "FFT.ipynb\n",
      ".virtual_documents\n",
      "Test_TF+FASTAI_conda_env.ipynb\n",
      "Music Genre Classification MLP.ipynb\n",
      ".ipynb_checkpoints\n",
      "Music Genre Classification Preprocess.ipynb\n",
      "data\n",
      "AmplitudeEnv+RMS.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "from fastai.basics import *\n",
    "from fastai.basics import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "import datetime \n",
    "import pytz  \n",
    "\n",
    "#!pip show fastai\n",
    "#print(datetime.datetime.now(pytz.timezone('Asia/Kolkata')))\n",
    "\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib \n",
    "from pathlib import Path\n",
    "\n",
    "#home_dir = Path(os.getcwd())\n",
    "home_dir = Path.cwd()\n",
    "print(f'\\n ** home_dir is ** : {home_dir} ')\n",
    "print(f'\\n ** home_dir.ls() ** \\n ')\n",
    "for i in range(len(home_dir.ls().items)):\n",
    "    print(home_dir.ls()[i].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = (home_dir/'data/gtzan/genres_original').ls()\n",
    "data_dir = Path(home_dir/'data/gtzan/genres_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : Use tf.data \n",
    "1. iterate thru all files 10 folders * 100 wav in each\n",
    "2. generate labels based on sub-directory name \n",
    "2. put all files and labels into a dataset\n",
    "3. listen to a few\n",
    "4. convert to mfcc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of files_ds <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "# of files in data_dir is 1000\n",
      "first 2 files in Dataset are \n",
      " [b'/Users/ganesh/Documents/GitHub/DLAudio/GS_Valerio_SoundofAI/data/gtzan/genres_original/blues/blues.00000.wav', b'/Users/ganesh/Documents/GitHub/DLAudio/GS_Valerio_SoundofAI/data/gtzan/genres_original/blues/blues.00001.wav']\n"
     ]
    }
   ],
   "source": [
    "#Create tf.data.Dataset from list of files\n",
    "#list_all_files = list(data_dir.glob('*/*'))\n",
    "files_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)\n",
    "print(f'Type of files_ds {type(files_ds)}') \n",
    "print(f'# of files in data_dir is {len(files_ds)}')\n",
    "#print(list_all_files[0:5])\n",
    "print(f'first 2 files in Dataset are \\n {[f.numpy() for f in files_ds.take(2)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes are ['blues' 'classical' 'country' 'disco' 'hiphop' 'jazz' 'metal' 'pop'\n",
      " 'reggae' 'rock']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted(\n",
    "    [item.name for item in data_dir.glob('*') if item.name != \".DS_Store\"])\n",
    "                      )\n",
    "print(f\"Classes are {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "#class_indices = [i+1 for i,_ in enumerate(class_names)]\n",
    "#print(f'Labels are {class_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got TensorSliceDataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-163f491bb6b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TESTING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got TensorSliceDataset)"
     ]
    }
   ],
   "source": [
    "#TESTING \n",
    "\n",
    "parts = tf.strings.split(Tensor(files_ds), os.path.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert file path to an (wav,label) pair\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == class_names\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_wav(audio):\n",
    "    pass \n",
    "    '''\n",
    "    audio, sample_rate = tf.audio.decode_wav(wav,\n",
    "                                             desired_channels=1,\n",
    "                                             desired_samples=44100)\n",
    "    return audio,sample_rate\n",
    "    \n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    '''\n",
    "    audio = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    '''\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "files_ds = files_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ParallelMapDataset' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-7a8440d1f5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParallelMapDataset' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "for i in files_ds.take(1):\n",
    "    print(files_ds.label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ds = tf.data.Dataset.from_tensor_slices(class_indices)\n",
    "    \n",
    "    return tf.data.Dataset.zip((files_ds, labels_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-f73b19e42d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_all_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-081875ca3602>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(files, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfiles_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlabels_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "ds = get_dataset(list_all_files,class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
